{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIC/TA File Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NoteBook shows examples of a workflow that determines the DIC and TA content of samples analysed on a VINDTA 3C (at UEA), and assesses the quality of measurements based on results for the Certified Reference Materials (CRMs) run on each analysis day. \n",
    "\n",
    "For questions, contact Elise S. Droste (e.droste@uea.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this NoteBook, prepare the file for post-analysis processing of dissolved inorganic carbon (DIC) and total alkalinity (TA) samples. \n",
    "\n",
    "\"Preparations\" include:\n",
    "- correcting filenaming mistakes during lab analyses, which may interfere with any automatic processing down the line (highly recommended)\n",
    "- adding auxiliary data from other datafiles, such as salinity (minimum requirement) and ideally also nutrients and any other co-collected data you might need later on (e.g. in situ temperature, pressure, ... ) (highly recommended)\n",
    "\n",
    "I'll be correcting mistakes in filenaming during lab analyses, inserting bottle data from the CTD cast, and nutrient data, as well as some other corrections. \n",
    "\n",
    "The final output of this notebook  will  be imported into `DIC_CRMs_QC.ipynb` Notebook where the DIC content will be determined. Output of that notebook will subsequently feed into the `TA_CRMs_QC.ipynb` NoteBook, where the TA content is determined. Output of that notebook will lastly feed into the `DICTA_samples_QC.ipynb` NoteBook, where the DIC and TA content for the samples will be quality checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore') # suppresses warning messages from packages\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # allows you to see multiple outputs from a single cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the raw data from the 3C VINDTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../rawdata/\"# directory where raw data is stored\n",
    "datafile = \"testrawdata_LucySummary_20240221.csv\" # filename \n",
    "\n",
    "read_kw = dict(header = 1, encoding = \"latin-1\") # required for Pandas to be able to read special symbols in the raw datafile \n",
    "\n",
    "vindta_data = pd.read_csv(Path(datadir) / datafile, **read_kw, na_values = -999) # read the datafile\n",
    "\n",
    "vindta_data.columns = vindta_data.columns.str.replace('Â°','') # remove the degree symbol, this will make importing the file later easier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Aborted runs and filenaming corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any filenaming corrections that need to be done, do them here. This includes removing aborted runs that contain no data. \n",
    "\n",
    "If the TA run was aborted but you have a valid DIC run, make sure not to remove the coulometer data! \n",
    "\n",
    "Steps: \n",
    "- Remove rows with no data due to aborted runs (and reindex)\n",
    "- Correct filenames in summary csv files\n",
    "- Also correct the associated station, cast, niskin, depth, replicate, duplicate numbers where relevant\n",
    "- Check if all associated .dat files can be found\n",
    "- Correct filenames in .dat filenames\n",
    "\n",
    "<font color=orange> Make sure that the corresponding .dat file is consistent with the filename i.e. Sample Name in the DataFrame! This will be checked in the next section. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# .dat filenaming corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .dat files contain the data for the TA determination. Check if all .dat files exist/can be found for each sample and CRM run. \n",
    "\n",
    "If they cannot be found, they need to be corrected manually in the specified directory where the .dat files are stored. \n",
    "\n",
    "<font color=orange> Do this only when a back up of the original files has been made elsewhere! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_datfiles(datdir, df):\n",
    "    # Loop through all runs that are sample runs or CRM runs (i.e. ignore the junk runs)\n",
    "    for filename in df[\"Sample Name\"].loc[df[\"Station\"] != 9999]:\n",
    "        datFile = filename + \".dat\"\n",
    "\n",
    "        # Check if the dat file exists \n",
    "        if os.path.isfile(datdir + \"/\" + datFile):\n",
    "            continue\n",
    "        else: \n",
    "            print(datFile + \" not found in specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8888_802_1208_0_1_1.dat not found in specified directory.\n",
      "8888_802_1208_0_1_2.dat not found in specified directory.\n",
      "0000_003_6_300_1_1.dat not found in specified directory.\n",
      "0000_003_6_300_2_1.dat not found in specified directory.\n",
      "0000_019_5_122_3_1.dat not found in specified directory.\n",
      "0000_019_21_6_1_1.dat not found in specified directory.\n",
      "7777_028_2_333_1_1.dat not found in specified directory.\n",
      "7777_028_2_333_2_1.dat not found in specified directory.\n",
      "7777_028_2_333_3_1.dat not found in specified directory.\n",
      "7777_028_4_290_1_1.dat not found in specified directory.\n",
      "7777_028_4_290_2_1.dat not found in specified directory.\n",
      "7777_028_4_290_3_1.dat not found in specified directory.\n",
      "7777_028_5_139_1_1.dat not found in specified directory.\n",
      "7777_028_6_159_1_1.dat not found in specified directory.\n",
      "7777_028_8_220_1_1.dat not found in specified directory.\n",
      "7777_028_8_220_2_1.dat not found in specified directory.\n",
      "7777_028_8_220_3_1.dat not found in specified directory.\n",
      "7777_028_9_80_1_1.dat not found in specified directory.\n",
      "7777_028_12_110_1_1.dat not found in specified directory.\n",
      "7777_028_13_200_1_1.dat not found in specified directory.\n",
      "7777_028_14_53_1_1.dat not found in specified directory.\n",
      "7777_028_16_35_1_1.dat not found in specified directory.\n",
      "7777_028_18_23_1_1.dat not found in specified directory.\n",
      "7777_028_19_15_1_1.dat not found in specified directory.\n",
      "7777_028_21_9_1_1.dat not found in specified directory.\n",
      "7777_028_22_5_1_1.dat not found in specified directory.\n",
      "2222_202401131194640_40_7_1_1.dat not found in specified directory.\n",
      "2222_202401131194640_12_7_1_1.dat not found in specified directory.\n",
      "2222_20240130190009_28_7_1_1.dat not found in specified directory.\n",
      "2222_20240201102130_43_7_1_1.dat not found in specified directory.\n",
      "2222_20240201102130_32_7_1_1.dat not found in specified directory.\n",
      "0000_036_4_300_3_1.dat not found in specified directory.\n",
      "0000_039_20_5_1_1.dat not found in specified directory.\n",
      "8888_0208_0028_0_1_1.dat not found in specified directory.\n",
      "8888_0208_0028_0_2_1.dat not found in specified directory.\n",
      "0000_076_12_230_1_1.dat not found in specified directory.\n",
      "2222_202401100013_1_7_1_1.dat not found in specified directory.\n",
      "2222_202401100013_1_7_2_1.dat not found in specified directory.\n",
      "0000_087_20_17_1_1.dat not found in specified directory.\n",
      "8888_0204_0414_0_1_1.dat not found in specified directory.\n",
      "8888_0204_0414_0_1_2.dat not found in specified directory.\n"
     ]
    }
   ],
   "source": [
    "datdir = \"../rawdata/\"\n",
    "check_datfiles(datdir = datdir, df = vindta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were are any .dat files not found, have they been corrected manually? \n",
    "\n",
    "\n",
    "<font color=green> YES /  </font> <font color=red> NO </font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = orange> Note: the output above shows that there are many .dat files not found. This is because this test case uses datafiles that have already been corrected for filenaming mistakes. If you have correctly corrected filenaming mistakes, then all .dat files should have been found, perhaps except for some aborted runs. You can just ignore those </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Pipette Volumes and Calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipette volumes should have been calibrated before and after analysis. The summary file will (should) have the pipette volumes as calibrated just before analyses. Decide here whether volumes have changed over time, or justify using different volumes. \n",
    "\n",
    "When were pipette calibrations done? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.8442])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([96.7556])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindta_data[\"DIC pipette volume (ml)\"].unique()\n",
    "vindta_data[\"TA pipette vol (ml)\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Insert auxiliary data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VINDTA software default values for Salinity and CTD Temp (C) are: Salinity= 35, CTD Temp (C) = 0. Nutrients ('Phosphate (uMol/Kg)','Silicate (uMol/Kg)', 'Nitrate (uMol/Kg)') are given a concentration of 0 umol/kg (nitrate is given -999/NAN?). \n",
    "\n",
    "If these data were unknown at the time of analysis and therefore have not been inserted into the VINDTA software during the run in the lab, you can add them here. \n",
    "\n",
    "Here, add measured values in these columns with the data from auxiliary datafiles (e.g. CTD bottle files). \n",
    "\n",
    "<font color=orange> Check for any missing data at the end </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Certified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According the the convention CRMs are identified with \"8888\" as the Station value. Their Sample Name is constructed as follows: \n",
    "\n",
    "> 8888_batch_CRMbottlenumber_repeat_replicate\n",
    "\n",
    "The class that is used below will help link up the CRMs with the relevant certified data (nutrients, salinity). \n",
    "\n",
    "It will also be used to calculate the DIC concentrations. \n",
    "\n",
    "Remember, here we're only looking at the CRMs. Quality checking will be done throughout the lab analysis period, but will only be finalised at the end. \n",
    "\n",
    "\n",
    "Check which CRM batch was used. Certified information on all batches can be found here: \n",
    "\n",
    "https://www.ncei.noaa.gov/access/ocean-carbon-data-system/oceans/Dickson_CRM/batches.html\n",
    "\n",
    "Record this information in a lookup file ('crm_batch_lookup.csv'), which will be used in the 'CalcDIC' Class (`DIC_CRMs_QC.ipynb`) to insert the certified information into the dataframe for further data processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([802, 208, 204])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindta_data[vindta_data[\"Station\"] == 8888][\"Cast\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange> Note: the output above shows that I have multiple batch numbers for CRMs in my raw datafile. These are in fact typo's made in the Software in the lab when setting up a run; all CRMs cam from batch 208. Normally, this should have been corrected earlier in this notebook, but it nicely illustrates that it's important to correct filenaming mistakes, to avoid confusion and accidentally using the wrong information </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Insert additional values (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also be a good place to insert any additional data required in further processing, such as any temperature measurements that were done in the lab of the sample, in addition to the temperature measurements made by the VINDTA system itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QC done in the next notebooks (i.e. `SD035_DIC_CRMs_QC.ipynb`, `SD035_TA_CRMs_QC.ipynb`, `SD035_DICTA_samples_QC.ipynb`) can expose other issues or necessary corrections in the datafile that need to be fixed before the final DIC and TA contents can be accurately determined. It can therefore be an iterative process. To streamline the process and enhance the transparency of the workflow and QC assessments, it is sometimes clearer to make any necessary formatting corrections in _this_ notebook/dataframe, instead of in any notebooks/dataframes down the processing line. In that way, all subsequent datafiles are consistent and changes made to the data are congregated in one place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Save the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file, which will be imported into `DIC_CRMs_QC.ipynb` for DIC determination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_datadir = Path(\"../output_data/fileprepped_rawdata/\")\n",
    "prepped_datafile = \"summary_prepped_\" + datetime.date.today().strftime(format = \"%Y%m%d\") + \".csv\"\n",
    "\n",
    "# vindta_data.to_csv(prepped_datadir / prepped_datafile, index = False) # uncomment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "âenv_dicta_kernelâ",
   "language": "python",
   "name": "env_dicta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
